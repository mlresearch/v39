<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo | ACML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Towards Maximum Likelihood: Learning Undirected Graphical Models
	using Persistent Sequential Monte Carlo">

  <meta name="citation_author" content="Xiong, Hanchen">

  <meta name="citation_author" content="Szedmak, Sandor">

  <meta name="citation_author" content="Piater, Justus">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Sixth Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="205">
<meta name="citation_lastpage" content="220">
<meta name="citation_pdf_url" content="xiong14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo</h1>

	<div id="authors">
	
		Hanchen Xiong,
	
		Sandor Szedmak,
	
		Justus Piater
	<br />
	</div>
	<div id="info">
		Proceedings of the Sixth Asian Conference on Machine Learning,
		pp. 205–220, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Along with the emergence of algorithms such as persistent contrastive divergence (PCD), tempered transition and parallel tempering, the past decade has witnessed a revival of learning undirected graphical models (UGMs) with sampling-based approximations. In this paper, based upon the analogy between Robbins-Monro’s stochastic approximation procedure and sequential Monte Carlo (SMC), we analyze the strengths and limitations of state-of-the-art learning algorithms from an SMC point of view. Moreover, we apply the rationale further in sampling at each iteration, and propose to learn UGMs using persistent sequential Monte Carlo (PSMC). The whole learning procedure is based on the samples from a long, persistent sequence of distributions which are actively constructed. Compared to the above-mentioned algorithms, one critical strength of PSMC- based learning is that it can explore the sampling space more effectively. In particular, it is robust when learning rates are large or model distributions are high-dimensional and thus multi-modal, which often causes other algorithms to deteriorate. We tested PSMC learning, also with other related methods, on carefully-designed experiments with both synthetic and real-word data, and our empirical results demonstrate that PSMC compares favorably with the state of the art.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="xiong14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
