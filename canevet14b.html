<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Sample Distillation for Object Detection and Image Classification | ACML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Sample Distillation for Object Detection and Image Classification">

  <meta name="citation_author" content="Canevet, Olivier">

  <meta name="citation_author" content="Lefakis, Leonidas">

  <meta name="citation_author" content="Fleuret, Francois">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Sixth Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="64">
<meta name="citation_lastpage" content="79">
<meta name="citation_pdf_url" content="canevet14b.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Sample Distillation for Object Detection and Image Classification</h1>

	<div id="authors">
	
		Olivier Canevet,
	
		Leonidas Lefakis,
	
		Francois Fleuret
	<br />
	</div>
	<div id="info">
		Proceedings of the Sixth Asian Conference on Machine Learning,
		pp. 64–79, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We propose a novel approach to efficiently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a “distillation” procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two different computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classification problem with artificial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance significantly.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="canevet14b.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
