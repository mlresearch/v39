<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Reinforcement learning with value advice | ACML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Reinforcement learning with value advice">

  <meta name="citation_author" content="Daswani, Mayank">

  <meta name="citation_author" content="Sunehag, Peter">

  <meta name="citation_author" content="Hutter, Marcus">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Sixth Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="299">
<meta name="citation_lastpage" content="314">
<meta name="citation_pdf_url" content="daswani14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Reinforcement learning with value advice</h1>

	<div id="authors">
	
		Mayank Daswani,
	
		Peter Sunehag,
	
		Marcus Hutter
	<br />
	</div>
	<div id="info">
		Proceedings of the Sixth Asian Conference on Machine Learning,
		pp. 299â€“314, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		The problem we consider in this paper is reinforcement learning with value advice. In this setting, the agent is given limited access to an oracle that can tell it the expected return (value) of any state-action pair with respect to the optimal policy. The agent must use this value to learn an explicit policy that performs well in the environment. We provide an algorithm called RLAdvice, based on the imitation learning algorithm DAgger. We illustrate the effectiveness of this method in the Arcade Learning Environment on three different games, using value estimates from UCT as advice.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="daswani14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
