---
title: Quasi Newton Temporal Difference Learning
abstract: Fast convergent and computationally inexpensive policy evaluation is an
  essential part of reinforcement learning algorithms based on policy iteration. Algorithms
  such as LSTD, LSPE, FPKF and NTD, have faster convergence rate but they are computationally
  slow. On the other hand, there are algorithms that are computationally fast but
  with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents
  a regularized Quasi Newton Temporal Difference learning algorithm which uses the
  second-order information while maintaining a fast convergence rate. In simple language,
  we combine the idea of TD learning with Quasi Newton algorithm SGD-QN. We explore
  the development of QNTD algorithm and discuss its convergence properties. We support
  our ideas with empirical results on 4 standard benchmarks in reinforcement learning
  literature with 2 small problems, Random Walk and Boyan chain and 2 bigger problems,
  cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up
  convergence and provides better accuracy in comparison to the conventional TD.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: givchi14
month: 0
firstpage: 159
lastpage: 172
page: 159-172
sections: 
author:
- given: Arash
  family: Givchi
- given: Maziar
  family: Palhang
date: 2015-02-16
address: Nha Trang City, Vietnam
publisher: PMLR
container-title: Proceedings of the Sixth Asian Conference on Machine Learning
volume: '39'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 2
  - 16
pdf: http://proceedings.mlr.press/v39/givchi14/givchi14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
