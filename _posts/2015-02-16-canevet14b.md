---
title: Sample Distillation for Object Detection and Image Classification
abstract: We propose a novel approach to efficiently select informative samples for
  large-scale learning. Instead of directly feeding a learning algorithm with a very
  large amount of samples, as it is usually done to reach state-of-the-art performance,
  we have developed a "distillation" procedure to recursively reduce the size of an
  initial training set using a criterion that ensures the maximization of the information
  content of the selected sub-set. We demonstrate the performance of this procedure
  for two different computer vision problems. First, we show that distillation can
  be used to improve the traditional bootstrapping approach to object detection. Second,
  we apply distillation to a classification problem with artificial distortions. We
  show that in both cases, using the result of a distillation process instead of a
  random sub-set taken uniformly in the original sample set improves performance significantly.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: canevet14b
month: 0
firstpage: 64
lastpage: 79
page: 64-79
sections: 
author:
- given: Olivier
  family: Canevet
- given: Leonidas
  family: Lefakis
- given: Francois
  family: Fleuret
date: 2015-02-16
address: Nha Trang City, Vietnam
publisher: PMLR
container-title: Proceedings of the Sixth Asian Conference on Machine Learning
volume: '39'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 2
  - 16
pdf: http://proceedings.mlr.press/v39/canevet14b/canevet14b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
